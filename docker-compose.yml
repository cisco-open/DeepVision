version: '3.9'
services:
  redis:
    image: redis
    container_name: "redis_vision"
    ports:
      - "6379:6379"
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
      interval: 10s
      timeout: 10s
      retries: 10
    profiles:
      - tracking
      - haidetection
      - optical_flow
  
  redistimeseries:
    image: redislabs/redistimeseries
    container_name: "redistimeseries_vision"
    ports:
      - "6380:6379"
    profiles:
      - tracking
      - haidetection
      - optical_flow

  grafana:
    build: ./grafana
    container_name: "grafana_vision"
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    profiles:
      - tracking
      - haidetection
      - optical_flow

  tracker:
    build:
      context: ./
      dockerfile: ./tracking/Dockerfile
    container_name: "tracker_vision"
    command: bash -c "python3 tracker.py ${MODEL_CONF} --redis ${REDIS_URL}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - tracking
      - haidetection
    environment:
      - DEFAULT_CACHE_DIR=/cache/mmtracker
      - XDG_CACHE_HOME=/cache/mmtracker
      - MMENGINE_HOME=/cache/mmtracker
      - PYTHONPATH=$PYTHONPATH:/shared_classes
    volumes:
      - cache_volume:/cache/mmtracker
      - ./utils:/shared_classes/utils

  optical_flow:
    build:
      context: ./
      dockerfile: ./tracking/Dockerfile
    container_name: "OF_vision"
    command: bash -c "python3 OpticalFlow.py ${FLOW_MODEL_CONF} --redis ${REDIS_URL} --opticalFlowRate ${OF_RATE} --frameDiff ${FRAME_DIFF} --maxlen ${MAXLEN} --grayScaleThreshold ${GRAY_SCALE_THRESHOLD}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - optical_flow
    environment:
      - PYTHONPATH=$PYTHONPATH:/shared_classes
    volumes:
      - ./utils:/shared_classes/utils

  actionrec:
    build:
      context: ./
      dockerfile: ./actionrecognition/Dockerfile
    container_name: "actionrec_vision"
    command: bash -c "python  actionrec.py ${AR_MODEL_CONF} --sampleSize ${AR_SAMPLE_SIZE} --batchSize ${AR_BATCH_SIZE} --redis ${REDIS_URL}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - haidetection
    environment:
      - DEFAULT_CACHE_DIR=/cache/mmaction
      - XDG_CACHE_HOME=/cache/mmaction
      - MMENGINE_HOME=/cache/mmaction
      - PYTHONPATH=$PYTHONPATH:/shared_classes
    volumes:
      - cache_volume:/cache/mmaction
      - ./utils:/shared_classes/utils


  producer:
    build: .
    container_name: "producer_vision"
    ports:
      - "5002:5001"
    command: bash -c "python3 producer.py -u ${REDIS_URL} --inputFps ${INPUT_FPS} --outputFps ${OUTPUT_FPS} ${INPUT_FILE} --webcam ${CAMERA} & python3 server.py -u ${REDIS_URL} --trackletLength ${TRACKLET_LENGTH} --score ${SCORE} --actionrecThreshold ${ACTION_REC_THRESHOLD} --videoThreshold ${VIDEO_THRESHOLD}"
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - haidetection
      - tracking
    environment:
      - MODEL_CONF=${MODEL_CONF}
      - PROFILE_NAME=${PROFILE_NAME}

  producer_of:
    build: .
    container_name: "producer_vision"
    ports:
      - "5002:5001"
    command: bash -c "python3 producer.py -u ${REDIS_URL} --inputFps ${INPUT_FPS} --outputFps ${OUTPUT_FPS} ${INPUT_FILE} --webcam ${CAMERA} & python3 OF_server.py -u ${REDIS_URL} --videoThreshold ${VIDEO_THRESHOLD} --minBoundary ${MIN_BOUNDARY} --maxBoundary ${MAX_BOUNDARY}"
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - optical_flow

  ona:
    build:
      context: ./cognitivesynergy/ONA
      dockerfile: Dockerfile
    image: ona 
    container_name: ONA
    restart: unless-stopped
    profiles:
      - ona
      - haidetection

  hybridaiobjectdetection:
    depends_on: 
      grafana:
        condition: service_started
      ona:
        condition: service_started
      producer:
        condition: service_started
      redis:
        condition: service_healthy
      redistimeseries:
        condition: service_started
      tracker:
        condition: service_started
    build: 
      context: ./cognitivesynergy
      dockerfile: Dockerfile
    image: cognitivesynergybase 
    container_name: hybridaiobjectdetection
    volumes:
      - ./cognitivesynergy:/app
      - /var/run/docker.sock:/var/run/docker.sock
      - ./utils:/shared_classes/utils
      - cache_volume:/cache/mmtracker
    environment:
      - PYTHONPATH=$PYTHONPATH:/shared_classes
    command: python3 main_detection.py
    stdin_open: true
    tty: true
    profiles:
      - haidetection

  hybridaiobjecttracking:
    depends_on: 
      - ona
    build: 
      context: .
      dockerfile: Dockerfile
    image: cognitivesynergybase 
    container_name: hybridaiobjecttracking
    volumes:
      - .:/app
    command: python main_tracking.py
    profiles:
      - haitracking

  hybridaistreamingtext:
    depends_on: 
      - ona
    build: .
    volumes:
      - .:/app
    command: python main_text.py
    profiles:
      - haitext

  hybridainetworking:
    depends_on: 
      - ona
    build: .
    volumes:
      - .:/app
    command: python main_networking.py
    profiles:
      - hainetworking

volumes:
  grafana-data:
    name: grafana-data
  cache_volume:
